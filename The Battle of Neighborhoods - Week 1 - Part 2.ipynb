{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# <ins>The Battle of Neighborhoods - Week 1</ins>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<br>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Data Section:"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Data Description"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "For any \"data science project\" data is of paramount importance. For this study, we will need data about hte neighborhoods in each of these metro cities.\n\nCities which will be analysed in this project: **New York City** and **London**.\n\nWe will be using the below datasets for analysing the cities:\n\n* **Data 1 (New York Dataset):**\n\n    Neighborhood has a total of 5 boroughs and 306 neighborhoods. In order to segement the neighborhoods and explore them, we will essentially need a dataset that contains the 5 boroughs and the neighborhoods that exist in each borough as well as the the latitude and logitude coordinates of each neighborhood.\n    \n    This dataset exists for free on the web. Link to the dataset is: https://geo.nyu.edu/catalog/nyu_2451_34572 . For convenience, the data is downloaded and the files are placed on the server. The following link has it: https://cocl.us/new_york_dataset\n    \n    The data present on the website is a **JSON** format data consisting of the attributes like type, id, coordinates (latitudes and longitudes), name, borough, neighborhood, stacked, annoline 1, annoline 2, annoline 3, etc. Since it contains many attributes we would not be extracting all of them. The attributes which we would be using/extracting from the dataset will be:\n    * **Borough**\n    * **Neighborhood**\n    * **Latitude**\n    * **Longitude**\n    \n  To prepare a datafram we will loop through the JSON data and store every parameter into their respective cells. To get the geographical coordinates i.e latitudes and longitudes, we can also use the ***Geopy*** library.\n  \n    \n* **Data 2 (London Dataset):**\n\n    Second data will be of the London dataset along with it's Boroughs. The dataset is not readily available on the web. Instead there is a Wikipedia page exists that has all the information we need to explore and cluster the neighborhoods. Hence, it will be required to scrape the Wikipedia page and wrangle the data, clean it, and then read it into a pandas dataframe so that it is in a structured format.\n    \n    The link for the Wikipedia page is as follows: https://en.wikipedia.org/wiki/List_of_areas_of_London\n    \n    Now, the data in the Wikipedia page has the following **columns**: Location, London Borough, Post town, Postcode district, Dial code and OS grid ref. Again, we won't be requiring the whole columns. The columns which would be needed are:\n    * **Location**\n    * **London Borough**\n    \n  For scraping the Wikipedia page we will use the ***Urllib*** and ***Beautiful Soup packages***.\n    \n  Here, the latitudes and longitudes we need to generate as it is not readily available on the Wikipedia page. We will use ***Geocoder Python package*** to get the latitude and longitude. "
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}